{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nhoQJai9njir"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sy2Tn7uHPPEN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet101V2,ResNet50V2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, LSTM, Input, Bidirectional, Flatten, Dense, Dropout, BatchNormalization,GlobalAveragePooling2D,TimeDistributed\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GzQPr0GSSVO",
        "outputId": "e6291162-b04c-4bd0-a10d-8405ff39df73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8R9o5h-oQ01",
        "outputId": "71875c02-d09c-4582-b182-43a167429647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "nframe=16\n",
        "curr_path = \"C:/Users/VISHWANATHAN VIVEK S/Desktop/Projects/VideoCaptioning\"\n",
        "test_path = curr_path + \"/Emotion_Detector.mp4\"\n",
        "vid=cv2.VideoCapture(test_path)\n",
        "frame_cnt = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "frame_count=0\n",
        "\n",
        "print(os.path.exists(curr_path + \"/frames\"))\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(curr_path + \"/frames\"):\n",
        "        os.makedirs(curr_path + \"/frames\")\n",
        "except OSError:\n",
        "    print ('Error: Creating directory of data')\n",
        "        \n",
        "while(True):\n",
        "    ret, frame = vid.read()\n",
        "    if ret :\n",
        "        if (frame_count<frame_cnt and frame_count%(frame_cnt//nframe)==0):\n",
        "            name = curr_path + '/frames' + '/' + str(frame_count//(frame_cnt//nframe)) + '.jpg'\n",
        "            print(name)\n",
        "            cv2.imwrite(name, frame)\n",
        "            frame_count+=1\n",
        "        else :\n",
        "            frame_count+=1\n",
        "            print(frame_count)\n",
        "            continue\n",
        "    else :\n",
        "        break\n",
        "    \n",
        "vid.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OrEy7K-rRB1P"
      },
      "outputs": [],
      "source": [
        "def enc_model(inputs,h_image,w_image):\n",
        "\n",
        "  base_model=ResNet50V2(include_top=False, input_shape=(h_image, w_image, 3),weights='imagenet')\n",
        "  model1=Sequential()\n",
        "  model1.add(TimeDistributed(base_model))\n",
        "  model1.add(TimeDistributed(GlobalAveragePooling2D()))\n",
        "  model1.add(TimeDistributed(Dropout(0.5)))\n",
        "  model1.add(TimeDistributed(Dense(1024,activation='relu')))\n",
        "  model1.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "  model1.add(Dropout(0.5))\n",
        "  model1.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "\n",
        "  for layers in base_model.layers[:-2]:\n",
        "    layers.trainable=False\n",
        "\n",
        "  enc_output=model1(inputs)\n",
        "  return enc_output #shape is batch_size, nframes, hidden_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ez-4S6ehRItH"
      },
      "outputs": [],
      "source": [
        "h_image=720\n",
        "w_image=1280\n",
        "nframe=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SlGOVrNRMym",
        "outputId": "eaf20ef9-4269-463f-a7f1-253d1c02cfa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94674944/94668760 [==============================] - 1s 0us/step\n",
            "94683136/94668760 [==============================] - 1s 0us/step\n",
            "(1, 16, 128)\n"
          ]
        }
      ],
      "source": [
        "print(enc_model(tf.random.normal([1,16,h_image,w_image,3]),h_image,w_image).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8oLTBboURT1R"
      },
      "outputs": [],
      "source": [
        "class pre_enc_model(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(pre_enc_model, self).__init__()\n",
        "    self.time_distributed1=TimeDistributed(Dense(1024,activation='relu'))\n",
        "    self.lstm1=Bidirectional(LSTM(256, return_sequences=True))\n",
        "    self.dropout=Dropout(0.5)\n",
        "    self.lstm2=Bidirectional(LSTM(64, return_sequences=True))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x=self.time_distributed1(inputs)\n",
        "    x=self.lstm1(x)\n",
        "    x=self.dropout(x)\n",
        "    enc_output=self.lstm2(x)\n",
        "    enc_hidden=enc_output[:,-1,:]\n",
        "\n",
        "    return enc_output, enc_hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzoY_O8ZRXJM",
        "outputId": "f61ff9e4-cfa4-41a6-ee20-c751765a3a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 80, 128) (2, 128)\n"
          ]
        }
      ],
      "source": [
        "model4=pre_enc_model()\n",
        "x,y=model4(tf.random.normal([2,80,4096]))\n",
        "print(x.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZKeqdoyMRhQN"
      },
      "outputs": [],
      "source": [
        "# def pre_enc_model(inputs):\n",
        "#   model2=Sequential()\n",
        "#   model2.add(TimeDistributed(Dense(1024,activation='relu')))\n",
        "#   model2.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "#   model2.add(Dropout(0.5))\n",
        "#   model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "\n",
        "#   enc_output1=model2(inputs)\n",
        "#   return enc_output1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1Hb5nl1GoVxg"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units=128):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1=Dense(units) \n",
        "    self.W2=Dense(units)\n",
        "    self.v=Dense(1)\n",
        "  def call(self, enc_output, pre_time_step_input):\n",
        "    # enc_output will be of shape (batch_size, max_seq_length, encoder_output_shape) encoder_output_shape is the length of output vector of each conv layer(here its 512)\n",
        "    # pre_time_step_input is of shape (batch_size, decoder_output_shape) decoder_output_shape!=encoder_output_shape and it's the the activation which we pass to next layer of LSTM\n",
        "    pre_time_step_input=tf.expand_dims(pre_time_step_input, axis=1) # To broadcast pre_time_step_input on each input timestep context vector \n",
        "    pre_softmax=self.v(tf.nn.tanh(self.W1(enc_output)+self.W2(pre_time_step_input))) #output shape is(batch_size, max_seq_length, 1 )\n",
        "    pre_softmax=tf.squeeze(pre_softmax, axis=2)\n",
        "    softmax_output=tf.nn.softmax(pre_softmax, axis=0)\n",
        "\n",
        "    context_vectors=tf.expand_dims(softmax_output, axis=2)*enc_output\n",
        "    context_vector=tf.reduce_sum(context_vectors, axis=1)\n",
        "\n",
        "    return context_vector #shape (batch_size, encoded_vector_output_shape (here its 128))\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO9w2yie29Yf",
        "outputId": "64b823bd-d146-4928-a498-60e3cc45d4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 128)\n"
          ]
        }
      ],
      "source": [
        "x=tf.random.normal([32,16,128])\n",
        "y=tf.random.normal([32,128])\n",
        "attention=Attention()\n",
        "context_vector=attention(x,y)\n",
        "print(context_vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XoeBfNNbohJo"
      },
      "outputs": [],
      "source": [
        "# class Decoder(tf.keras.layers.Layer):\n",
        "#   def __init__(self, attention, p=0.5, embedding_size=300, vocab_size=10000, hidden_size=128, num_layers=1):\n",
        "#     super(Decoder, self).__init__()\n",
        "#     self.hidden_size=hidden_size\n",
        "#     self.num_layers=num_layers # number of layers of LSTM stacked over each other\n",
        "#     self.attention=attention #attention will be instantiated later on\n",
        "#     self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "#     self.lstm=LSTM(hidden_size, return_sequences=True, dropout=0.1, return_state=True)\n",
        "#     self.dense=Dense(vocab_size, activation='softmax')\n",
        "#     self.dropout=Dropout(p)\n",
        "\n",
        "#   def call(self, x, encoder_states, pre_time_step_input, cell_state):\n",
        "\n",
        "#     x=tf.expand_dims(x,1)\n",
        "#     embedding_vec = self.dropout(self.embedding(x)) #shape is batch_size,1,embedding_size\n",
        "#     context_vector = self.attention(encoder_states, pre_time_step_input) \n",
        "#     context_vector=tf.expand_dims(context_vector, 1) #shape is batch_size, 1, hidden_size\n",
        "#     context_vec=tf.concat([embedding_vec, context_vector], axis=-1)\n",
        "#     _, pre_time_step_input , cell_state=self.lstm(context_vec, initial_state=[pre_time_step_input, cell_state])\n",
        "#     out=tf.concat([pre_time_step_input, tf.squeeze(context_vec, axis=1)], axis=-1)\n",
        "#     output=self.dense(out)\n",
        "\n",
        "#     return output, pre_time_step_input, cell_state\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZsO2kZ_5UH2C"
      },
      "outputs": [],
      "source": [
        "# class Decoder(tf.keras.layers.Layer):\n",
        "#   def __init__(self, attention, p=0.5, embedding_size=300, vocab_size=10000, hidden_size=128, num_layers=1):\n",
        "#     super(Decoder, self).__init__()\n",
        "#     self.hidden_size=hidden_size\n",
        "#     self.num_layers=num_layers # number of layers of LSTM stacked over each other\n",
        "#     self.attention=attention #attention will be instantiated later on\n",
        "#     self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "#     self.lstm=LSTM(hidden_size, return_sequences=True, dropout=0.1, return_state=True)\n",
        "#     self.dense=Dense(vocab_size, activation='softmax')\n",
        "#     self.dropout=Dropout(p)\n",
        "\n",
        "#   def call(self, x, encoder_states, pre_time_step_input, cell_state):\n",
        "\n",
        "#     x=tf.expand_dims(x,1)\n",
        "#     embedding_vec = self.dropout(self.embedding(x)) #shape is batch_size,1,embedding_size\n",
        "#     context_vector = self.attention(encoder_states, pre_time_step_input) \n",
        "#     context_vector=tf.expand_dims(context_vector, 1) #shape is batch_size, 1, hidden_size\n",
        "#     context_vec=tf.concat([embedding_vec, context_vector], axis=-1)\n",
        "#     _, pre_time_step_input , cell_state=self.lstm(context_vec, initial_state=[pre_time_step_input, cell_state])\n",
        "#     out=tf.concat([pre_time_step_input, tf.squeeze(context_vec, axis=1)], axis=-1)\n",
        "#     output=self.dense(out)\n",
        "\n",
        "#     return output, pre_time_step_input, cell_state\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nXC3dTEmojUP"
      },
      "outputs": [],
      "source": [
        "# attention=Attention()\n",
        "# decoder=Decoder(attention, 0.5, 300, 10000, 128, 1)\n",
        "\n",
        "# x=tf.random.uniform(shape=([32]), minval=1, maxval=32, dtype=tf.int32)\n",
        "# encoder_states=tf.random.normal([32,16,128])\n",
        "# pre_time_step_input=tf.random.normal([32,128])\n",
        "# cell_state=tf.zeros([32,128])\n",
        "\n",
        "# a,b,c=decoder(x, encoder_states, pre_time_step_input, cell_state)\n",
        "# print(a.shape,b.shape,c.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size=10000, embedding_dim=300, dec_units=128, batch_sz=32, num_layers=1, drop_prob=0.1):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.num_layers = num_layers\n",
        "        self.drop_prob = drop_prob\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # multiple number of layers\n",
        "        self.gru_layers = []\n",
        "        for i in range(self.num_layers):\n",
        "            self.gru_layers.append(tf.keras.layers.GRU(self.dec_units, return_state = True, return_sequences=True,\n",
        "                                    recurrent_initializer='glorot_uniform',\n",
        "                                    dropout = (0 if self.num_layers==1 else self.drop_prob)))\n",
        "        self.fc = Dense(vocab_size)\n",
        "\n",
        "        # attention\n",
        "        self.attention = Attention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        # enc output shape = (batch_sz, maxlen, hiddensize)\n",
        "        context_vector = self.attention(enc_output, hidden)\n",
        "\n",
        "        # x shape after embedding ( batchsize, 1, embedding_dim)\n",
        "        # x=tf.argmax(x, axis=-1)\n",
        "        x = self.embedding(x)\n",
        "        # x shape after concatenation = (batchsize, 1, embedding_dim + hiddensize)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        # passing the concatenated vector to gru\n",
        "        output, state = self.gru_layers[0](x, initial_state = hidden)\n",
        "        for i in range(1, self.num_layers-1):\n",
        "            output, state = self.gru_layers[i](output)\n",
        "        # output shape = (batchsize*1, hiddensize)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        # output shape = (batchsize, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state\n",
        "\n",
        "\n",
        "# class Decoder(tf.keras.layers.Layer):\n",
        "#   def __init__(self, attention, p=0.5, embedding_size=300, vocab_size=10000, hidden_size=128, num_layers=1):\n",
        "#     super(Decoder, self).__init__()\n",
        "#     self.hidden_size=hidden_size\n",
        "#     self.num_layers=num_layers # number of layers of LSTM stacked over each other\n",
        "#     self.attention=attention #attention will be instantiated later on\n",
        "#     self.embedding=tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "#     self.lstm=LSTM(hidden_size, return_sequences=True, dropout=0.1, return_state=True)\n",
        "#     self.dense=Dense(vocab_size, activation='softmax')\n",
        "#     self.dropout=Dropout(p)\n",
        "\n",
        "#   def call(self, x, encoder_states, pre_time_step_input, cell_state):\n",
        "\n",
        "#     x=tf.expand_dims(x,1)\n",
        "#     embedding_vec = self.dropout(self.embedding(x)) #shape is batch_size,1,embedding_size\n",
        "#     context_vector = self.attention(encoder_states, pre_time_step_input) \n",
        "#     context_vector=tf.expand_dims(context_vector, 1) #shape is batch_size, 1, hidden_size\n",
        "#     context_vec=tf.concat([embedding_vec, context_vector], axis=-1)\n",
        "#     _, pre_time_step_input , cell_state=self.lstm(context_vec, initial_state=[pre_time_step_input, cell_state])\n",
        "#     out=tf.concat([pre_time_step_input, tf.squeeze(context_vec, axis=1)], axis=-1)\n",
        "#     output=self.dense(out)\n",
        "\n",
        "    # return output, pre_time_step_input, cell_state\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S6cliO5stMDx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=pre_enc_model()\n",
        "decoder = Decoder()\n",
        "sample_decoder_output, _ = decoder(tf.random.uniform((32, 1)),\n",
        "                                      tf.zeros([32,128]), tf.random.normal([32,80,128]))\n",
        "print(sample_decoder_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfofTdIBuBvZ",
        "outputId": "4343bfa0-244d-4976-cf3a-3f731c5d9dfd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrt7y68-oltj",
        "outputId": "30e96896-127e-4c54-930c-df9882c55fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19714\n",
            "4649\n",
            "19714\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1450"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "train_path='/content/drive/MyDrive/training_data'\n",
        "TRAIN_LABEL_PATH = '/content/drive/MyDrive/training_label.json'\n",
        "\n",
        "# loading the json file for training\n",
        "with open(TRAIN_LABEL_PATH) as data_file:    \n",
        "    y_data = json.load(data_file)\n",
        "# train_list contains all the captions with their video ID\n",
        "# vocab_list contains all the vocabulary from training data\n",
        "train_list = []\n",
        "vocab_list = []\n",
        "for y in y_data:\n",
        "  for caption in y['caption']:\n",
        "    caption = \"<bos> \" + caption + \" <eos>\"\n",
        "    # we are only using sentences whose length lie between 5 and 12\n",
        "    if len(caption.split())>=12 or len(caption.split())<5:\n",
        "      continue\n",
        "    else:\n",
        "      train_list.append([caption, y['id']])\n",
        "print(len(train_list))\n",
        "random.shuffle(train_list)\n",
        "training_list = train_list\n",
        "for train in training_list:\n",
        "    vocab_list.append(train[0])\n",
        "# Tokenizing the words\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(vocab_list)\n",
        "print(len(tokenizer.word_index))\n",
        "x_data = {}\n",
        "TRAIN_FEATURE_DIR = os.path.join(train_path, 'feat')\n",
        "# Loading all the numpy arrays at once and saving them in a dictionary\n",
        "for filename in os.listdir(TRAIN_FEATURE_DIR):\n",
        "    f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename))\n",
        "    x_data[filename[:-4]] = f # -4 to remove .npy from the file names\n",
        "print(len(training_list))\n",
        "len(x_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiWwD_RmBcaJ",
        "outputId": "488a8ce0-e11b-4c69-aa2a-7217e4fe3934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19714, 2)\n"
          ]
        }
      ],
      "source": [
        "print(np.shape(train_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dmN7qdmR9qrd"
      },
      "outputs": [],
      "source": [
        "train_list=train_list[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_CwI7C98pD-e"
      },
      "outputs": [],
      "source": [
        "def load_datatest(train_path, epochs=100, x_data=x_data, tokenizer=tokenizer, num_decoder_tokens=10000,training_list=train_list, batch_size=32, maxlen=12):\n",
        "    encoder_input_data = []\n",
        "    decoder_input_data = []\n",
        "    decoder_target_data = []\n",
        "    videoId = []\n",
        "    videoSeq = []\n",
        "    # separating the videoId and the video captions\n",
        "    for idx, cap in enumerate(training_list):\n",
        "        caption = cap[0]\n",
        "        videoId.append(cap[1])\n",
        "        videoSeq.append(caption)\n",
        "    # converting the captions to tokens and padding them to equal sizes\n",
        "    train_sequences = tokenizer.texts_to_sequences(videoSeq)\n",
        "    train_sequences = np.array(train_sequences)\n",
        "    train_sequences = pad_sequences(train_sequences, padding='post',truncating='post', maxlen=maxlen)\n",
        "    max_seq_length = train_sequences.shape[1]\n",
        "    filesize = len(train_sequences)\n",
        "    X_data = []\n",
        "    y_data = []\n",
        "    vCount = 0\n",
        "    n = 0\n",
        "    for i in range(epochs):\n",
        "      for idx in  range(0,filesize):\n",
        "        n += 1\n",
        "        encoder_input_data.append(x_data[videoId[idx]])\n",
        "        y = to_categorical(train_sequences[idx], num_decoder_tokens)\n",
        "        # decoder_input_data.append(y[:-1])\n",
        "        decoder_target_data.append(y[1:])\n",
        "        if n == batch_size:\n",
        "          encoder_input = np.array(encoder_input_data)\n",
        "          # decoder_input = np.array(decoder_input_data)\n",
        "          decoder_target = np.array(decoder_target_data)\n",
        "          encoder_input_data = []\n",
        "          # decoder_input_data = []\n",
        "          decoder_target_data = []\n",
        "          n = 0\n",
        "          # yield ([encoder_input, decoder_input], decoder_target)\n",
        "          yield (encoder_input, decoder_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_datatest(train_path, epochs=100, x_data=x_data, tokenizer=tokenizer, num_decoder_tokens=10000,training_list=train_list, batch_size=32, maxlen=12):\n",
        "#     encoder_input_data = []\n",
        "#     decoder_input_data = []\n",
        "#     decoder_target_data = []\n",
        "#     videoId = []\n",
        "#     videoSeq = []\n",
        "#     # separating the videoId and the video captions\n",
        "#     for idx, cap in enumerate(training_list):\n",
        "#         caption = cap[0]\n",
        "#         videoId.append(cap[1])\n",
        "#         videoSeq.append(caption)\n",
        "#     # converting the captions to tokens and padding them to equal sizes\n",
        "#     train_sequences = tokenizer.texts_to_sequences(videoSeq)\n",
        "#     train_sequences = np.array(train_sequences)\n",
        "#     train_sequences = pad_sequences(train_sequences, padding='post',truncating='post', maxlen=maxlen)\n",
        "#     max_seq_length = train_sequences.shape[1]\n",
        "#     filesize = len(train_sequences)\n",
        "#     X_data = []\n",
        "#     y_data = []\n",
        "#     vCount = 0\n",
        "#     n = 0\n",
        "#     for idx in  range(0,filesize):\n",
        "#       n += 1\n",
        "#       encoder_input_data.append(x_data[videoId[idx]])\n",
        "#       y = to_categorical(train_sequences[idx], num_decoder_tokens)\n",
        "#       # decoder_input_data.append(y[:-1])\n",
        "#       decoder_target_data.append(y[1:])\n",
        "#       if n == batch_size:\n",
        "#         encoder_input = np.array(encoder_input_data)\n",
        "#         # decoder_input = np.array(decoder_input_data)\n",
        "#         decoder_target = np.array(decoder_target_data)\n",
        "#         encoder_input_data = []\n",
        "#         # decoder_input_data = []\n",
        "#         decoder_target_data = []\n",
        "#         n = 0\n",
        "#         # yield ([encoder_input, decoder_input], decoder_target)\n",
        "#         yield (encoder_input, decoder_target)\n"
      ],
      "metadata": {
        "id": "pPpwBH6K7rjo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DxrIHUcJNwDd"
      },
      "outputs": [],
      "source": [
        "train= load_datatest(train_path=train_path ,batch_size=32, training_list=train_list, x_data=x_data, epochs=150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8uuRwnUZn-g",
        "outputId": "991296bf-98f9-4ca8-9342-f857a1669ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 80, 4096), (32, 11, 10000))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "next(train)[0].shape, next(train)[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.001\n",
        "decoder_learning_rate=1\n",
        "clip=2\n",
        "\n",
        "enc_optimizer = tf.keras.optimizers.Adam(lr=learning_rate, \n",
        "                                         clipnorm = clip)\n",
        "dec_optimizer = tf.keras.optimizers.Adam(lr=learning_rate*decoder_learning_rate,\n",
        "                                         clipnorm = clip)\n",
        "\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits = False)\n",
        "\n",
        "def loss_fn(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huWJFjpfuLMJ",
        "outputId": "15cee1ea-9ca3-4b16-e491-88687d003387"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teach_force=0.5\n",
        "teacher_forcing_ratio = teach_force\n",
        "@tf.function\n",
        "def train_step(inp, rep):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape:\n",
        "        enc_output, enc_hidden = encoder(inp)\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        # dec_input = tf.expand_dims([vocab['SOS']]*BATCH_SIZE, 1)\n",
        "        dec_input=tf.random.uniform((32, 1))\n",
        "        \n",
        "        \n",
        "        # using teacher_forcing_ratio as a parameter \n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "        if use_teacher_forcing:\n",
        "            # teacher forcing - feeding the reply as the next input\n",
        "            for t in range(1, rep.shape[1]):\n",
        "                preds, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "                loss += loss_fn(rep[:,t,:], preds)\n",
        "                # teacher forcing\n",
        "                dec_input = tf.expand_dims(tf.argmax(rep[:, t,:], -1),1)\n",
        "                \n",
        "\n",
        "        else:\n",
        "            # without teacher forcing using the decoder output as next input\n",
        "            for t in range(1, rep.shape[1]):\n",
        "                preds, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "                loss += loss_fn(rep[:, t,:], preds)\n",
        "\n",
        "                # passing the output\n",
        "                dec_input = tf.expand_dims(tf.math.argmax(preds, axis=1), 1)\n",
        "                \n",
        "\n",
        "                \n",
        "\n",
        "    batch_loss = (loss/ int(rep.shape[1]))\n",
        "    \n",
        "    enc_variables = encoder.trainable_variables\n",
        "    dec_variables = decoder.trainable_variables\n",
        "\n",
        "    enc_grads = enc_tape.gradient(loss, enc_variables)\n",
        "    dec_grads = dec_tape.gradient(loss, dec_variables)\n",
        "\n",
        "    enc_optimizer.apply_gradients(zip(enc_grads, enc_variables))\n",
        "    dec_optimizer.apply_gradients(zip(dec_grads, dec_variables))\n",
        "\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "sI8Rl59Aubqh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_step(next(train)[0], next(train)[1]))\n",
        "# print(train_step(train.next()[0], train.next()[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxqPhHXZuhnx",
        "outputId": "53f35041-a596-4661-cef7-aee9270b1c77"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.0009131866, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "ep = 350\n",
        "losses = []\n",
        "val_losses = []\n",
        "num_batches = np.shape(train_list)[0] // 32\n",
        "for epoch in range(ep):\n",
        "    print(f\"\\nSTARTING EPOCH: {epoch+1}\")\n",
        "    start = time.time()\n",
        "    enc_hidden = tf.zeros([32, 128])\n",
        "    total_loss = 0\n",
        "    val_loss_batch = []\n",
        "\n",
        "    for (batch,(inp, rep)) in enumerate(train):\n",
        "        batch_loss = train_step(inp, rep)\n",
        "        total_loss += batch_loss\n",
        "        losses.append(batch_loss)\n",
        "\n",
        "        # if (batch+1)%print_every == 0:\n",
        "        #     print(\"\\t Batch {}/{} ===> Train Loss {:.4f}\".format(batch+1,num_batches,batch_loss.numpy()))\n",
        "    \n",
        "    # dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "    print(\"Epoch {} completed with Training Loss {:.4f} in {:.4f} sec\".format(epoch+1, \n",
        "                                                                      total_loss,\n",
        "                                                                     time.time() - start)) \n",
        "# total_loss/steps_per_epoch,                   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "WbWRd8XtuiQr",
        "outputId": "a275141d-8570-42e0-83f1-f5ff71c76726"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STARTING EPOCH: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-885ed4edde20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# ep = 3\n",
        "# losses = []\n",
        "# val_losses = []\n",
        "# num_batches = np.shape(train_list)[0] // 32\n",
        "# for epoch in range(ep):\n",
        "#     print(f\"\\nSTARTING EPOCH: {epoch+1}\")\n",
        "#     start = time.time()\n",
        "#     enc_hidden = tf.zeros([32, 128])\n",
        "#     total_loss = 0\n",
        "#     val_loss_batch = []\n",
        "\n",
        "#     for (inp, rep) in train:\n",
        "#         batch_loss = train_step(inp, rep)\n",
        "#         total_loss += batch_loss\n",
        "#         losses.append(batch_loss)\n",
        "\n",
        "#         # if (batch+1)%print_every == 0:\n",
        "#         #     print(\"\\t Batch {}/{} ===> Train Loss {:.4f}\".format(batch+1,num_batches,batch_loss.numpy()))\n",
        "    \n",
        "#     # dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "#     print(\"Epoch {} completed with Training Loss {:.4f} in {:.4f} sec\".format(epoch+1, \n",
        "#                                                                       total_loss,\n",
        "#                                                                      time.time() - start)) \n",
        "# # total_loss/steps_per_epoch,                   "
      ],
      "metadata": {
        "id": "SSKUoR2PxXL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWrylJmsBL2o"
      },
      "outputs": [],
      "source": [
        "# outputs=tf.Variable(tf.zeros([32, 12, 10000]))\n",
        "# # outputs=tf.zeros([32, 12, 10000])\n",
        "# class Seq2Seq(tf.keras.layers.Layer):\n",
        "#   def __init__(self, decoder, encoder):\n",
        "#         super(Seq2Seq, self).__init__()\n",
        "#         self.encoder = encoder\n",
        "#         self.decoder = decoder\n",
        "#   def call(self, source, target, hidden_size=128, teacher_force_ratio=0.5, vocab_size=10000, batch_size=32):\n",
        "\n",
        "#     target_len=target.shape[1]\n",
        "#     # outputs = tf.zeros([batch_size, target_len, vocab_size])\n",
        "#     # outputs=tf.Variable(tf.zeros([batch_size, target_len, vocab_size]))\n",
        "#     encoder_states=self.encoder(source)\n",
        "#     # encoder_states=pre_enc_model(source)\n",
        "#     x = tf.argmax(target[:,0,:] , axis=-1)\n",
        "#     pre_time_step_input=tf.zeros([batch_size, hidden_size])\n",
        "#     cell_state=tf.zeros([batch_size, hidden_size])\n",
        "#     for t in range(1, target_len):\n",
        "#       output, pre_time_step_input, cell_state = self.decoder(x, encoder_states, pre_time_step_input, cell_state)\n",
        "#       # outputs[:,t,:] = output\n",
        "#       outputs[:,t,:].assign(output)\n",
        "#       best_guess = tf.argmax(output , axis=-1)\n",
        "#       x = tf.argmax(target[:,t,:] , axis=-1) if random.random() < teacher_force_ratio else best_guess\n",
        "#     return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXMGr7_ESvPr"
      },
      "outputs": [],
      "source": [
        "# outputs=tf.Variable(tf.zeros([32, 12, 10000]))\n",
        "# # outputs=tf.zeros([32, 12, 10000])\n",
        "# class Seq2Seq(tf.keras.layers.Layer):\n",
        "  \n",
        "#   def call(self, source, target, hidden_size=128, teacher_force_ratio=0.5, vocab_size=10000, batch_size=32):\n",
        "\n",
        "#     target_len=target.shape[1]\n",
        "#     # outputs = tf.zeros([batch_size, target_len, vocab_size])\n",
        "#     # outputs=tf.Variable(tf.zeros([batch_size, target_len, vocab_size]))\n",
        "#     encoder_states=pre_enc_model()(source)\n",
        "#     # encoder_states=pre_enc_model(source)\n",
        "#     x = tf.argmax(target[:,0,:] , axis=-1)\n",
        "#     pre_time_step_input=tf.zeros([batch_size, hidden_size])\n",
        "#     cell_state=tf.zeros([batch_size, hidden_size])\n",
        "#     for t in range(1, target_len):\n",
        "#       output, pre_time_step_input, cell_state = Decoder()(x, encoder_states, pre_time_step_input, cell_state)\n",
        "#       # outputs[:,t,:] = output\n",
        "#       outputs[:,t,:].assign(output)\n",
        "#       best_guess = tf.argmax(output , axis=-1)\n",
        "#       x = tf.argmax(target[:,t,:] , axis=-1) if random.random() < teacher_force_ratio else best_guess\n",
        "#     return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWb6hm6aVdja"
      },
      "outputs": [],
      "source": [
        "# vocab_size=10000\n",
        "# embedding_size = 300\n",
        "# hidden_size = 128\n",
        "# num_layers = 2\n",
        "# dec_dropout = 0.2\n",
        "\n",
        "# time_steps_encoder=80\n",
        "# num_encoder_tokens=4096\n",
        "# time_steps_decoder=12\n",
        "\n",
        "# encoder_net = pre_enc_model()\n",
        "# attention = Attention()\n",
        "# decoder_net = Decoder(attention, dec_dropout ,embedding_size, vocab_size, hidden_size, num_layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvYHOg3LTGlW"
      },
      "outputs": [],
      "source": [
        "# vocab_size=10000\n",
        "# embedding_size = 300\n",
        "# hidden_size = 128\n",
        "# num_layers = 2\n",
        "# dec_dropout = 0.2\n",
        "\n",
        "# time_steps_encoder=80\n",
        "# num_encoder_tokens=4096\n",
        "# time_steps_decoder=12\n",
        "\n",
        "# encoder_net = pre_enc_model()\n",
        "# attention = Attention()\n",
        "# decoder_net = Decoder(dec_dropout ,embedding_size, vocab_size, hidden_size, num_layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUrbEO4O4VZG"
      },
      "outputs": [],
      "source": [
        "# encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")\n",
        "# target= Input(shape=(time_steps_decoder, vocab_size), name=\"decoder_inputs\")\n",
        "# seq2seq = Seq2Seq(decoder_net , encoder_net)\n",
        "# outputs=seq2seq(encoder_inputs, target)\n",
        "# print(outputs.shape)\n",
        "# # model=Model(encoder_inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiolHOD64awn"
      },
      "outputs": [],
      "source": [
        "# tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNLGGnwx2gpF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Rv31_IDt5vcw"
      },
      "outputs": [],
      "source": [
        "# class Model1(tf.keras.Model):\n",
        "#   def __init__(self, decoder_net, encoder_net):\n",
        "#     super(Model1, self).__init__()\n",
        "#     self.seq2seq=Seq2Seq(decoder_net, encoder_net)\n",
        "\n",
        "#   def call(self, inputs, target):\n",
        "#     x=self.seq2seq(inputs, target)\n",
        "#     return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3u59tb6WYWqo"
      },
      "outputs": [],
      "source": [
        "# model5=Model1(decoder_net, encoder_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Qk0X_zYzYg_r"
      },
      "outputs": [],
      "source": [
        "# output=model5(encoder_inputs, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-deXd9gmTFj-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oyoKglaBYttW"
      },
      "outputs": [],
      "source": [
        "# outputs=tf.Variable(tf.zeros([32, 12, 10000]))\n",
        "# # outputs=tf.zeros([32, 12, 10000])\n",
        "# class Seq2Seq(tf.keras.layers.Layer):\n",
        "#   def __init__(self, decoder, encoder):\n",
        "#         super(Seq2Seq, self).__init__()\n",
        "#         self.encoder = encoder\n",
        "#         self.decoder = decoder\n",
        "#   def call(self, source, target_len=12, hidden_size=128, teacher_force_ratio=0.5, vocab_size=10000, batch_size=32):\n",
        "\n",
        "#     target_len=target_len\n",
        "#     # outputs = tf.zeros([batch_size, target_len, vocab_size])\n",
        "#     # outputs=tf.Variable(tf.zeros([batch_size, target_len, vocab_size]))\n",
        "#     encoder_states=self.encoder(source)\n",
        "#     # encoder_states=pre_enc_model(source)\n",
        "#     # x = tf.argmax(target[:,0,:] , axis=-1)\n",
        "#     x=tf.squeeze(tf.argmax(tf.zeros([batch_size,1,vocab_size],dtype=tf.dtypes.float32), axis=-1), axis=-1)\n",
        "#     pre_time_step_input=tf.zeros([batch_size, hidden_size])\n",
        "#     cell_state=tf.zeros([batch_size, hidden_size])\n",
        "#     for t in range(1, target_len):\n",
        "#       output, pre_time_step_input, cell_state = self.decoder(x, encoder_states, pre_time_step_input, cell_state)\n",
        "#       # outputs[:,t,:] = output\n",
        "#       outputs[:,t,:].assign(output)\n",
        "#       best_guess = tf.argmax(output , axis=-1)\n",
        "#       # x = tf.argmax(target[:,t,:] , axis=-1) if random.random() < teacher_force_ratio else best_guess\n",
        "#       x=best_guess\n",
        "#     return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hYvykqK3aOm-"
      },
      "outputs": [],
      "source": [
        "# encoder_inputs = Input(shape=(time_steps_encoder, num_encoder_tokens), name=\"encoder_inputs\")\n",
        "# target= Input(shape=(time_steps_decoder, vocab_size), name=\"decoder_inputs\")\n",
        "# seq2seq = Seq2Seq(decoder_net , encoder_net)\n",
        "# outputs=seq2seq(encoder_inputs)\n",
        "# print(outputs.shape)\n",
        "# # model=Model(encoder_inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NtE6GYqrN1rq"
      },
      "outputs": [],
      "source": [
        "# model=Model(encoder_inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "6tZI1yxtAG3-"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-l0zjw9pBwGt"
      },
      "outputs": [],
      "source": [
        "# tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wjNMO8e5CI_I"
      },
      "outputs": [],
      "source": [
        "# my_callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience = 5, verbose=1, mode='min'),\n",
        "#               tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, monitor='loss', patience = 5, verbose=1, mode='min' )]\n",
        "# opt = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GxmEuSs1DyH4"
      },
      "outputs": [],
      "source": [
        "# model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n",
        "# try:\n",
        "#     model.fit(train, epochs=350, steps_per_epoch=(len(training_list)//32), callbacks=my_callbacks)\n",
        "# except KeyboardInterrupt:\n",
        "#     print(\"\\nW: interrupt received, stopping\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "H11F0FeoEbSX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MAsNjmIIOUhq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "PuWvOKaNPDf5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "w-Yru1bpQpee"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ATxn02hqVy6x"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "yx6jW29aXUWR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jJFzjZ069Q6r"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Vid_captioning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}